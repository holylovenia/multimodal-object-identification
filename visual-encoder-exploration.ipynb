{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491731e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "from copy import deepcopy\n",
    "\n",
    "import datasets\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Union, Any, Optional, List, Tuple\n",
    "from detr import CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9209299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolyTrainer(transformers.Trainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (`Lst[str]`, *optional*):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels:\n",
    "            labels = inputs['labels']\n",
    "#             labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "#             if len(labels) == 1:\n",
    "#                 labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                loss = loss.mean().detach()\n",
    "\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                else:\n",
    "                    logits = outputs[1:]\n",
    "            else:\n",
    "                loss = None\n",
    "                with self.compute_loss_context_manager():\n",
    "                    outputs = model(**inputs)\n",
    "                if isinstance(outputs, dict):\n",
    "                    logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                # TODO: this needs to be fixed and made cleaner later.\n",
    "                if self.args.past_index >= 0:\n",
    "                    self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = transformers.trainer_pt_utils.nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03819c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f0e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/detr-resnet-50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f3366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sneakers_2(Clone)',\n",
       " 'prefabPath': 'clothing_store/models/prefab/sneakers_2',\n",
       " 'bbox': [1178, 564, 29, 22],\n",
       " 'position': [-3.576437473297119, 0.03797276318073273, 2.1606364250183105]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_bbox = json.loads(open(\"./simmc2/data/public/cloth_store_1_1_1_bbox.json\").read())\n",
    "public_bbox[\"Items\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22211003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefab_path': '1208725/Jacket_blue',\n",
       "  'unique_id': 0,\n",
       "  'index': 0,\n",
       "  'bbox': [1303, 88, 218, 153],\n",
       "  'position': [-2.866098403930664, 0.05314898490905762, 1.6969716548919678]},\n",
       " {'prefab_path': '1208725/Jacket_blue',\n",
       "  'unique_id': 0,\n",
       "  'index': 1,\n",
       "  'bbox': [1916, 21, 289, 181],\n",
       "  'position': [-1.1282265186309814, 0.05314898490905762, 2.7818620204925537]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/polo shirt 18',\n",
       "  'unique_id': 1,\n",
       "  'index': 2,\n",
       "  'bbox': [947, 166, 139, 54],\n",
       "  'position': [-5.126088619232178, 1.8119479417800903, 2.2323215007781982]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_3_06',\n",
       "  'unique_id': 2,\n",
       "  'index': 3,\n",
       "  'bbox': [1045, 132, 182, 39],\n",
       "  'position': [-6.755237102508545, -0.7599018812179565, 4.868677139282227]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_2_06',\n",
       "  'unique_id': 3,\n",
       "  'index': 4,\n",
       "  'bbox': [1065, 156, 148, 20],\n",
       "  'position': [-6.0655694007873535, -2.101559638977051, -1.9722471237182617]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_10 1',\n",
       "  'unique_id': 4,\n",
       "  'index': 5,\n",
       "  'bbox': [1025, 131, 189, 60],\n",
       "  'position': [-3.961207866668701, 1.4082798957824707, 4.4196882247924805]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_10 1',\n",
       "  'unique_id': 4,\n",
       "  'index': 6,\n",
       "  'bbox': [1076, 123, 199, 43],\n",
       "  'position': [-3.6342077255249023, 1.4082798957824707, 4.422687530517578]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_4_05',\n",
       "  'unique_id': 5,\n",
       "  'index': 7,\n",
       "  'bbox': [1129, 116, 201, 32],\n",
       "  'position': [-4.468832015991211, 2.188898801803589, -0.7671551704406738]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_10 1',\n",
       "  'unique_id': 4,\n",
       "  'index': 8,\n",
       "  'bbox': [1147, 110, 214, 56],\n",
       "  'position': [-3.225207805633545, 1.4082798957824707, 4.4196882247924805]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_06',\n",
       "  'unique_id': 6,\n",
       "  'index': 9,\n",
       "  'bbox': [1188, 103, 222, 56],\n",
       "  'position': [-2.8060922622680664, 0.6689841747283936, 3.1460695266723633]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/polo shirt 36',\n",
       "  'unique_id': 7,\n",
       "  'index': 10,\n",
       "  'bbox': [1220, 124, 169, 40],\n",
       "  'position': [-3.4826784133911133, 1.905760407447815, 2.245088577270508]},\n",
       " {'prefab_path': '1208725/Jacket_blue',\n",
       "  'unique_id': 0,\n",
       "  'index': 11,\n",
       "  'bbox': [1248, 109, 197, 26],\n",
       "  'position': [-3.8888626098632812, 0.05314898490905762, 2.2177734375]},\n",
       " {'prefab_path': 'clothing_store/models/prefab/trousers_2',\n",
       "  'unique_id': 8,\n",
       "  'index': 12,\n",
       "  'bbox': [945, 336, 164, 58],\n",
       "  'position': [-5.106999397277832, 0.23014092445373535, 2.1661055088043213]},\n",
       " {'prefab_path': 'clothing_store/models/prefab/trousers_4',\n",
       "  'unique_id': 9,\n",
       "  'index': 13,\n",
       "  'bbox': [1039, 340, 181, 77],\n",
       "  'position': [-4.373917579650879, 0.23014092445373535, 2.2122232913970947]},\n",
       " {'prefab_path': '1514019/Rearranged/Jeans_Blue_V4',\n",
       "  'unique_id': 10,\n",
       "  'index': 14,\n",
       "  'bbox': [1171, 348, 203, 75],\n",
       "  'position': [-3.613121747970581, 0.24453043937683105, 2.2062571048736572]},\n",
       " {'prefab_path': 'clothing_store/models/prefab/trousers_4',\n",
       "  'unique_id': 9,\n",
       "  'index': 15,\n",
       "  'bbox': [1299, 352, 184, 120],\n",
       "  'position': [-2.897865056991577, 0.23014092445373535, 2.189257860183716]},\n",
       " {'prefab_path': 'clothing_store/models/prefab/trousers',\n",
       "  'unique_id': 11,\n",
       "  'index': 16,\n",
       "  'bbox': [1900, 377, 139, 197],\n",
       "  'position': [-1.1222920417785645, 0.2301410436630249, 2.24706768989563]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_1',\n",
       "  'unique_id': 12,\n",
       "  'index': 17,\n",
       "  'bbox': [1375, 441, 80, 70],\n",
       "  'position': [-2.266435146331787, -2.1309332847595215, -1.2333738803863525]},\n",
       " {'prefab_path': '1208725/Tshirt_black',\n",
       "  'unique_id': 13,\n",
       "  'index': 18,\n",
       "  'bbox': [1428, 460, 47, 41],\n",
       "  'position': [-2.5947325229644775,\n",
       "   -0.49289125204086304,\n",
       "   -0.18958348035812378]},\n",
       " {'prefab_path': '1208725/Jacket_red',\n",
       "  'unique_id': 14,\n",
       "  'index': 19,\n",
       "  'bbox': [1513, 436, 63, 31],\n",
       "  'position': [-2.9410252571105957, 1.8819808959960938, 2.145235538482666]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_3_06',\n",
       "  'unique_id': 2,\n",
       "  'index': 20,\n",
       "  'bbox': [1530, 401, 153, 122],\n",
       "  'position': [-5.19620418548584, -1.8049025535583496, 2.051140308380127]},\n",
       " {'prefab_path': '1208725/Jacket_red',\n",
       "  'unique_id': 14,\n",
       "  'index': 21,\n",
       "  'bbox': [1694, 447, 71, 27],\n",
       "  'position': [-1.2898681163787842, 1.8819806575775146, 2.1549997329711914]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_4_09',\n",
       "  'unique_id': 15,\n",
       "  'index': 22,\n",
       "  'bbox': [1885, 433, 109, 117],\n",
       "  'position': [-1.9055615663528442, 0.7694786190986633, 0.1380312144756317]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_12',\n",
       "  'unique_id': 16,\n",
       "  'index': 23,\n",
       "  'bbox': [1982, 424, 133, 115],\n",
       "  'position': [-1.0638165473937988, -0.5723289847373962, 0.4042286276817322]},\n",
       " {'prefab_path': '870230/hoodie03',\n",
       "  'unique_id': 17,\n",
       "  'index': 24,\n",
       "  'bbox': [1691, 480, 75, 209],\n",
       "  'position': [-1.9966552257537842, 1.0199635028839111, 0.2162725329399109]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_10',\n",
       "  'unique_id': 18,\n",
       "  'index': 25,\n",
       "  'bbox': [1910, 447, 189, 186],\n",
       "  'position': [-2.0007622241973877, 0.8372488021850586, -0.20933988690376282]},\n",
       " {'prefab_path': 'clothing_store/models/prefab/sneakers_2',\n",
       "  'unique_id': 19,\n",
       "  'index': 26,\n",
       "  'bbox': [1178, 564, 29, 22],\n",
       "  'position': [-3.576437473297119, 0.03797276318073273, 2.1606364250183105]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/long_coat_12',\n",
       "  'unique_id': 16,\n",
       "  'index': 27,\n",
       "  'bbox': [1198, 403, 385, 507],\n",
       "  'position': [-1.9855897426605225, -0.5723292231559753, 0.6882750988006592]},\n",
       " {'prefab_path': 'MensCollection/Prefabs/Rearranged/jacket_2_03',\n",
       "  'unique_id': 20,\n",
       "  'index': 28,\n",
       "  'bbox': [1612, 485, 155, 77],\n",
       "  'position': [-0.07216310501098633, -2.349472999572754, 1.1521568298339844]},\n",
       " {'prefab_path': '870230/hoodie03',\n",
       "  'unique_id': 17,\n",
       "  'index': 29,\n",
       "  'bbox': [1684, 548, 240, 291],\n",
       "  'position': [-2.0913333892822266, 1.0199635028839111, -0.23972150683403015]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_scene = json.loads(open(\"./simmc2/data/public/cloth_store_1_1_1_scene.json\").read())\n",
    "public_scene[\"scenes\"][0][\"objects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2d48e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand': 'North Lodge',\n",
       " 'color': 'black',\n",
       " 'customerRating': 3.7,\n",
       " 'materials': 'wood',\n",
       " 'price': '$549',\n",
       " 'type': 'EndTable'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_prefab = json.loads(open(\"./simmc2/data/furniture_prefab_metadata_all.json\").read())\n",
    "fashion_prefab[\"modified_wayfair_assets/EndTable/EndTable_ATGR2663_v6_LOD2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "877b1d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assetType': 'jacket_hanging',\n",
       " 'customerReview': 4.8,\n",
       " 'availableSizes': ['XL', 'S', 'M', 'XXL'],\n",
       " 'color': 'blue',\n",
       " 'pattern': 'plain',\n",
       " 'brand': 'Global Voyager',\n",
       " 'sleeveLength': 'full',\n",
       " 'type': 'jacket',\n",
       " 'price': 39.99,\n",
       " 'size': 'S'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_prefab = json.loads(open(\"./simmc2/data/fashion_prefab_metadata_all.json\").read())\n",
    "fashion_prefab[\"MensCollection/Prefabs/Rearranged/jacket_3_06\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68edb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categories(\n",
    "    fashion_prefab_path=\"./simmc2/data/fashion_prefab_metadata_all.json\",\n",
    "    furniture_prefab_path=\"./simmc2/data/furniture_prefab_metadata_all.json\",\n",
    "    return_simple_mapping=True):\n",
    "    \n",
    "    fashion_prefab = json.loads(open(fashion_prefab_path).read())\n",
    "    furniture_prefab = json.loads(open(furniture_prefab_path).read())\n",
    "    \n",
    "    categories = []\n",
    "    fashion_categories = list(set([fashion_prefab[item][\"type\"] for item in fashion_prefab]))\n",
    "    furniture_categories = list(set([furniture_prefab[item][\"type\"] for item in furniture_prefab]))\n",
    "    category_mapping = {}\n",
    "    \n",
    "    for i in range(len(fashion_categories)):\n",
    "        categories.append({\n",
    "            \"supercategory\": \"fashion\",\n",
    "            \"id\": i,\n",
    "            \"name\": fashion_categories[i],\n",
    "        })\n",
    "            \n",
    "        \n",
    "    current_num_categories = len(categories)\n",
    "    for i in range(len(furniture_categories)):\n",
    "        categories.append({\n",
    "            \"supercategory\": \"furniture\",\n",
    "            \"id\": i,\n",
    "            \"name\": furniture_categories[i],\n",
    "        })\n",
    "        \n",
    "    if return_simple_mapping:\n",
    "        id2cat = dict(enumerate(fashion_categories + furniture_categories))\n",
    "        cat2id = {v: k for k, v in id2cat.items()}\n",
    "        return {\"categories\": categories, \"id2cat\": id2cat, \"cat2id\": cat2id}\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4d1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = load_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b56f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objects_in_scenes_dataset(\n",
    "    img_dir_paths=[\n",
    "        \"./simmc2/data/simmc2_scene_images_dstc10_public_part1\",\n",
    "        \"./simmc2/data/simmc2_scene_images_dstc10_public_part2\"],\n",
    "    scene_dir_path=\"./simmc2/data/public\",\n",
    "    fashion_prefab_path=\"./simmc2/data/fashion_prefab_metadata_all.json\",\n",
    "#     include_fashion_attrs=[\"type\", \"color\", \"pattern\", \"sleeveLength\"],\n",
    "    furniture_prefab_path=\"./simmc2/data/furniture_prefab_metadata_all.json\",\n",
    "#     include_furniture_attrs=[\"type\", \"color\", \"materials\"],\n",
    "    mapping=MAPPING,\n",
    "    ):\n",
    "    \n",
    "    data_dict = {\n",
    "        \"image\": [], \"image_id\": [], \"objects\": []}\n",
    "    \n",
    "    fashion_prefab = json.loads(open(fashion_prefab_path).read())\n",
    "    furniture_prefab = json.loads(open(furniture_prefab_path).read())\n",
    "    scene2id = {}\n",
    "    \n",
    "    for img_dir_path in img_dir_paths:\n",
    "        \n",
    "        for img_file_id in os.listdir(img_dir_path):\n",
    "            img_file_path = os.path.join(img_dir_path, img_file_id)\n",
    "            scene_id = img_file_id.split(\".\")[0]                \n",
    "            scene_file_path = os.path.join(scene_dir_path, f\"{scene_id}_scene.json\")\n",
    "            \n",
    "            if os.path.isfile(scene_file_path):\n",
    "                data_dict[\"image\"].append(img_file_path)\n",
    "                num_scenes = len(data_dict[\"image_id\"])\n",
    "                scene2id[scene_id] = num_scenes\n",
    "                data_dict[\"image_id\"].append(num_scenes)\n",
    "                \n",
    "                scene_json = json.loads(open(scene_file_path).read())\n",
    "                scene_objects = scene_json[\"scenes\"][0][\"objects\"]\n",
    "                objects = []\n",
    "                for scene_object in scene_objects:\n",
    "                    object_annotation = {\n",
    "                        \"bbox\": [float(b) for b in scene_object[\"bbox\"]],\n",
    "                        \"id\": scene_object[\"unique_id\"],\n",
    "                        \"area\": None,\n",
    "                        \"segmentation\": [],\n",
    "                        \"iscrowd\": False,\n",
    "                    }\n",
    "                    if fashion_prefab.get(scene_object[\"prefab_path\"]) is not None:\n",
    "#                         for f_attr in include_fashion_attrs:\n",
    "#                             object_annotation[f_attr] = fashion_prefab[scene_object[\"prefab_path\"]][f_attr]\n",
    "                        item = fashion_prefab[scene_object[\"prefab_path\"]]\n",
    "                    else:\n",
    "                        item = furniture_prefab[scene_object[\"prefab_path\"]]\n",
    "                    object_annotation[\"category_id\"] = mapping[\"cat2id\"][item[\"type\"]]\n",
    "                    objects.append(object_annotation)\n",
    "                data_dict[\"objects\"].append(objects)\n",
    "            \n",
    "    dataset = datasets.Dataset.from_dict(data_dict)\n",
    "    dataset = dataset.cast_column(\"image\", datasets.Image(decode=True))\n",
    "    \n",
    "    id2scene = {v: k for k, v in scene2id.items()}\n",
    "    mapping[\"scene2id\"] = scene2id\n",
    "    mapping[\"id2scene\"] = id2scene\n",
    "    \n",
    "    return dataset, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05e6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, MAPPING = load_objects_in_scenes_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42479181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_id', 'objects'],\n",
       "        num_rows: 924\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_id', 'objects'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f482c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_area(example_batch):\n",
    "    width, height = example_batch[\"image\"].size\n",
    "    area = width * height\n",
    "    for i in range(len(example_batch[\"objects\"])):\n",
    "        example_batch[\"objects\"][i][\"area\"] = area\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15f1957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1697708c9fd24eb5a45f81485525744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1156 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(compute_image_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb9b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1886x1005>,\n",
       " 'image_id': 0,\n",
       " 'objects': [{'area': 1895430,\n",
       "   'bbox': [198.0, 139.0, 212.0, 127.0],\n",
       "   'category_id': 11,\n",
       "   'id': 0,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [311.0, 189.0, 156.0, 76.0],\n",
       "   'category_id': 6,\n",
       "   'id': 1,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [384.0, 193.0, 174.0, 60.0],\n",
       "   'category_id': 4,\n",
       "   'id': 2,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1167.0, 205.0, 175.0, 47.0],\n",
       "   'category_id': 16,\n",
       "   'id': 3,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1200.0, 202.0, 164.0, 67.0],\n",
       "   'category_id': 14,\n",
       "   'id': 4,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1139.0, 236.0, 130.0, 30.0],\n",
       "   'category_id': 6,\n",
       "   'id': 5,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1563.0, 236.0, 320.0, 151.0],\n",
       "   'category_id': 4,\n",
       "   'id': 6,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [330.0, 346.0, 195.0, 70.0],\n",
       "   'category_id': 16,\n",
       "   'id': 7,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [401.0, 363.0, 147.0, 65.0],\n",
       "   'category_id': 11,\n",
       "   'id': 8,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1720.0, 230.0, 352.0, 164.0],\n",
       "   'category_id': 6,\n",
       "   'id': 9,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [214.0, 345.0, 231.0, 102.0],\n",
       "   'category_id': 4,\n",
       "   'id': 10,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1139.0, 368.0, 168.0, 27.0],\n",
       "   'category_id': 15,\n",
       "   'id': 11,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1170.0, 374.0, 182.0, 33.0],\n",
       "   'category_id': 15,\n",
       "   'id': 11,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1209.0, 380.0, 180.0, 42.0],\n",
       "   'category_id': 15,\n",
       "   'id': 12,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [774.0, 483.0, 34.0, 58.0],\n",
       "   'category_id': 15,\n",
       "   'id': 11,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [449.0, 565.0, 27.0, 75.0],\n",
       "   'category_id': 9,\n",
       "   'id': 13,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [420.0, 583.0, 17.0, 80.0],\n",
       "   'category_id': 9,\n",
       "   'id': 14,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [477.0, 560.0, 13.0, 65.0],\n",
       "   'category_id': 9,\n",
       "   'id': 15,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [561.0, 558.0, 11.0, 61.0],\n",
       "   'category_id': 9,\n",
       "   'id': 14,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [536.0, 566.0, 19.0, 75.0],\n",
       "   'category_id': 13,\n",
       "   'id': 16,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [513.0, 578.0, 20.0, 83.0],\n",
       "   'category_id': 13,\n",
       "   'id': 17,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [868.0, 471.0, 241.0, 27.0],\n",
       "   'category_id': 15,\n",
       "   'id': 18,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [838.0, 497.0, 203.0, 46.0],\n",
       "   'category_id': 15,\n",
       "   'id': 19,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [318.0, 631.0, 27.0, 108.0],\n",
       "   'category_id': 13,\n",
       "   'id': 20,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [363.0, 614.0, 21.0, 90.0],\n",
       "   'category_id': 9,\n",
       "   'id': 15,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [389.0, 598.0, 20.0, 87.0],\n",
       "   'category_id': 9,\n",
       "   'id': 21,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [447.0, 624.0, 37.0, 96.0],\n",
       "   'category_id': 9,\n",
       "   'id': 22,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [471.0, 608.0, 27.0, 95.0],\n",
       "   'category_id': 13,\n",
       "   'id': 23,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [503.0, 593.0, 18.0, 79.0],\n",
       "   'category_id': 13,\n",
       "   'id': 24,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [738.0, 494.0, 249.0, 19.0],\n",
       "   'category_id': 15,\n",
       "   'id': 25,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [707.0, 505.0, 251.0, 31.0],\n",
       "   'category_id': 15,\n",
       "   'id': 26,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [744.0, 494.0, 234.0, 114.0],\n",
       "   'category_id': 15,\n",
       "   'id': 27,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [916.0, 492.0, 237.0, 24.0],\n",
       "   'category_id': 15,\n",
       "   'id': 28,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [740.0, 521.0, 284.0, 119.0],\n",
       "   'category_id': 15,\n",
       "   'id': 26,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1483.0, 615.0, 387.0, 182.0],\n",
       "   'category_id': 17,\n",
       "   'id': 29,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [1796.0, 728.0, 275.0, 88.0],\n",
       "   'category_id': 7,\n",
       "   'id': 30,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [571.0, 878.0, 101.0, 58.0],\n",
       "   'category_id': 14,\n",
       "   'id': 4,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [583.0, 857.0, 146.0, 155.0],\n",
       "   'category_id': 6,\n",
       "   'id': 5,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [696.0, 831.0, 172.0, 76.0],\n",
       "   'category_id': 6,\n",
       "   'id': 5,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [769.0, 802.0, 201.0, 58.0],\n",
       "   'category_id': 9,\n",
       "   'id': 31,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [838.0, 866.0, 137.0, 88.0],\n",
       "   'category_id': 6,\n",
       "   'id': 32,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [894.0, 895.0, 99.0, 82.0],\n",
       "   'category_id': 14,\n",
       "   'id': 33,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [541.0, 936.0, 67.0, 61.0],\n",
       "   'category_id': 14,\n",
       "   'id': 34,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []},\n",
       "  {'area': 1895430,\n",
       "   'bbox': [918.0, 936.0, 67.0, 48.0],\n",
       "   'category_id': 14,\n",
       "   'id': 35,\n",
       "   'iscrowd': False,\n",
       "   'segmentation': []}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921775e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_id', 'objects'],\n",
       "        num_rows: 936\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_id', 'objects'],\n",
       "        num_rows: 116\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'image_id', 'objects'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = dataset.train_test_split(0.1)\n",
    "res = raw_datasets[\"train\"].train_test_split(0.1)\n",
    "raw_datasets[\"train\"] = res[\"train\"]\n",
    "raw_datasets[\"valid\"] = res[\"test\"]\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e385e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 1658210,\n",
       " 'bbox': [932.0, 18.0, 340.0, 38.0],\n",
       " 'category_id': 11,\n",
       " 'id': 1,\n",
       " 'iscrowd': False,\n",
       " 'segmentation': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][100][\"objects\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a789f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = transformers.AutoFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e0f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    images = [image.convert(\"RGB\") for image in example_batch[\"image\"]]\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": object_} \\\n",
    "        for (id_, object_) in zip(example_batch[\"image_id\"], example_batch[\"objects\"])\n",
    "    ]\n",
    "    return feature_extractor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9463c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_datasets = deepcopy(raw_datasets)\n",
    "proc_datasets[\"train\"] = proc_datasets[\"train\"].with_transform(transform)\n",
    "proc_datasets[\"valid\"] = proc_datasets[\"valid\"].with_transform(transform)\n",
    "proc_datasets[\"test\"] = proc_datasets[\"test\"].with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4948cc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.0801, -0.0801, -0.0801,  ..., -1.4672, -1.4329, -1.4158],\n",
       "          [-0.0801, -0.0801, -0.0801,  ..., -1.3987, -1.4329, -1.4158],\n",
       "          [-0.0801, -0.0801, -0.0801,  ..., -1.3815, -1.3815, -1.3815],\n",
       "          ...,\n",
       "          [-0.2513, -0.2513, -0.2513,  ..., -0.8335, -0.8335, -0.8335],\n",
       "          [-0.2513, -0.2513, -0.2513,  ..., -0.8335, -0.8335, -0.8164],\n",
       "          [-0.2342, -0.2513, -0.2513,  ..., -0.8335, -0.8335, -0.8335]],\n",
       " \n",
       "         [[-0.0049, -0.0049, -0.0049,  ..., -1.5105, -1.5105, -1.4755],\n",
       "          [-0.0049, -0.0049,  0.0126,  ..., -1.4580, -1.4930, -1.4930],\n",
       "          [ 0.0301,  0.0126,  0.0126,  ..., -1.4405, -1.4580, -1.4755],\n",
       "          ...,\n",
       "          [-0.1625, -0.1625, -0.1625,  ..., -0.7577, -0.7577, -0.7577],\n",
       "          [-0.1450, -0.1450, -0.1625,  ..., -0.7577, -0.7577, -0.7402],\n",
       "          [-0.1450, -0.1450, -0.1625,  ..., -0.7577, -0.7577, -0.7577]],\n",
       " \n",
       "         [[ 0.1651,  0.1651,  0.1651,  ..., -1.3164, -1.2990, -1.2990],\n",
       "          [ 0.1651,  0.1825,  0.1825,  ..., -1.2816, -1.2990, -1.3164],\n",
       "          [ 0.1651,  0.1651,  0.1651,  ..., -1.2641, -1.2816, -1.2990],\n",
       "          ...,\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.6193, -0.6193, -0.6193],\n",
       "          [-0.0092, -0.0092, -0.0092,  ..., -0.6193, -0.6193, -0.6193],\n",
       "          [ 0.0082, -0.0092, -0.0092,  ..., -0.6193, -0.6193, -0.6367]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': {'boxes': tensor([[0.5960, 0.3576, 0.0495, 0.0241],\n",
       "          [0.5157, 0.4500, 0.0376, 0.0696],\n",
       "          [0.5672, 0.4127, 0.0481, 0.0203],\n",
       "          [0.5586, 0.4177, 0.0443, 0.0203],\n",
       "          [0.5486, 0.4171, 0.0424, 0.0139],\n",
       "          [0.5515, 0.4241, 0.0395, 0.0203],\n",
       "          [0.5738, 0.4228, 0.0376, 0.0127],\n",
       "          [0.5696, 0.4190, 0.0434, 0.0203],\n",
       "          [0.5867, 0.4114, 0.0510, 0.0228],\n",
       "          [0.6105, 0.4095, 0.0500, 0.0190],\n",
       "          [0.5896, 0.4222, 0.0443, 0.0266],\n",
       "          [0.6039, 0.4108, 0.0500, 0.0190],\n",
       "          [0.6139, 0.4190, 0.0424, 0.0177],\n",
       "          [0.6279, 0.4443, 0.0495, 0.0911],\n",
       "          [0.7370, 0.4437, 0.0553, 0.1025],\n",
       "          [0.7899, 0.4044, 0.0600, 0.0291],\n",
       "          [0.8001, 0.4063, 0.0605, 0.0329],\n",
       "          [0.8149, 0.4247, 0.0510, 0.0443],\n",
       "          [0.8097, 0.4133, 0.0605, 0.0468],\n",
       "          [0.8444, 0.4253, 0.0519, 0.0481],\n",
       "          [0.8633, 0.4209, 0.0657, 0.0722],\n",
       "          [0.8485, 0.4342, 0.0524, 0.0658],\n",
       "          [0.8816, 0.4228, 0.0576, 0.0557],\n",
       "          [0.9159, 0.4424, 0.0681, 0.1228],\n",
       "          [0.5217, 0.5753, 0.0476, 0.0646],\n",
       "          [0.5591, 0.5722, 0.0481, 0.0481],\n",
       "          [0.5972, 0.5759, 0.0510, 0.0506],\n",
       "          [0.6346, 0.5835, 0.0524, 0.0557],\n",
       "          [0.6589, 0.6057, 0.0105, 0.0342],\n",
       "          [0.6698, 0.5981, 0.0086, 0.0114],\n",
       "          [0.6768, 0.5835, 0.0091, 0.0101],\n",
       "          [0.6858, 0.5911, 0.0091, 0.0101],\n",
       "          [0.7051, 0.6019, 0.0095, 0.0215],\n",
       "          [0.6960, 0.5873, 0.0105, 0.0152],\n",
       "          [0.7651, 0.6253, 0.0152, 0.0582],\n",
       "          [0.7885, 0.6253, 0.0162, 0.0608],\n",
       "          [0.8109, 0.6494, 0.0124, 0.0785],\n",
       "          [0.6684, 0.6411, 0.0638, 0.0823],\n",
       "          [0.6863, 0.6367, 0.0691, 0.0785],\n",
       "          [0.7113, 0.6190, 0.0762, 0.0506],\n",
       "          [0.7423, 0.6000, 0.0572, 0.0684],\n",
       "          [0.7970, 0.6120, 0.0581, 0.0747],\n",
       "          [0.8514, 0.6209, 0.0591, 0.0873],\n",
       "          [0.8699, 0.6608, 0.0124, 0.0785],\n",
       "          [0.9121, 0.6329, 0.0596, 0.0937],\n",
       "          [0.9166, 0.6810, 0.0143, 0.1089],\n",
       "          [0.9697, 0.6772, 0.0138, 0.0962],\n",
       "          [0.5939, 0.7070, 0.0195, 0.0291],\n",
       "          [0.5791, 0.7487, 0.0195, 0.0443],\n",
       "          [0.5765, 0.7209, 0.0067, 0.0468],\n",
       "          [0.6115, 0.7171, 0.0157, 0.0089],\n",
       "          [0.6263, 0.6918, 0.0195, 0.0063],\n",
       "          [0.6424, 0.7424, 0.0195, 0.0215],\n",
       "          [0.6129, 0.7323, 0.0043, 0.0494],\n",
       "          [0.6582, 0.7475, 0.0195, 0.0443],\n",
       "          [0.7263, 0.6747, 0.0710, 0.1190],\n",
       "          [0.7277, 0.6418, 0.0834, 0.0506],\n",
       "          [0.7406, 0.6424, 0.0967, 0.0823],\n",
       "          [0.7468, 0.6639, 0.0815, 0.1000],\n",
       "          [0.5591, 0.8133, 0.0214, 0.1532],\n",
       "          [0.6632, 0.7734, 0.0238, 0.0532],\n",
       "          [0.6713, 0.7722, 0.0210, 0.0380],\n",
       "          [0.6251, 0.8266, 0.0858, 0.1139],\n",
       "          [0.7668, 0.8911, 0.0148, 0.0861],\n",
       "          [0.8032, 0.8373, 0.0076, 0.0266],\n",
       "          [0.5829, 0.8778, 0.0815, 0.1937],\n",
       "          [0.6510, 0.8266, 0.0777, 0.0709],\n",
       "          [0.6634, 0.8766, 0.0824, 0.1962],\n",
       "          [0.7730, 0.9443, 0.0205, 0.1114]]),\n",
       "  'class_labels': tensor([16, 11, 14,  9, 14, 11,  4, 14, 16, 14, 14, 14,  4, 14, 14, 16, 16, 11,\n",
       "          14, 11, 16, 11, 14, 16, 17,  7,  7,  7, 14,  9, 16, 14, 14, 16,  8,  8,\n",
       "           8, 14, 16, 16,  7,  5,  7,  8,  7,  8,  8, 16, 16,  1,  4, 16, 14,  1,\n",
       "          16, 11, 14, 16, 14, 11, 16, 16, 14,  1,  1, 11, 11, 16,  1]),\n",
       "  'image_id': tensor([707]),\n",
       "  'area': tensor([669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166., 669166., 669166., 669166.,\n",
       "          669166., 669166., 669166., 669166., 669166.]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'orig_size': tensor([ 790, 2099]),\n",
       "  'size': tensor([ 502, 1333])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_datasets[\"valid\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d44898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = feature_extractor.pad_and_create_pixel_mask(\n",
    "        pixel_values, return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d7c011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([29, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([29]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForObjectDetection.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    id2label=MAPPING[\"id2cat\"],\n",
    "    label2id=MAPPING[\"cat2id\"],\n",
    "    ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9493e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_datasets[\"train\"] = proc_datasets[\"train\"].shard(num_shards=50, index=0)\n",
    "proc_datasets[\"valid\"] = proc_datasets[\"valid\"].shard(num_shards=50, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6824ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=f\"./save/vis-enc/{model_name_or_path}_1e-4_bs32\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=1,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=RANDOM_SEED,\n",
    "    data_seed=RANDOM_SEED,\n",
    "    load_best_model_at_end=True,\n",
    "#     dataloader_num_workers=4,\n",
    "    overwrite_output_dir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "391b3ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = HolyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=proc_datasets[\"train\"],\n",
    "    eval_dataset=proc_datasets[\"valid\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44f16de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/holy/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/10 : < :, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.277700</td>\n",
       "      <td>5.886050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./save/vis-enc/facebook/detr-resnet-50_1e-4_bs32/checkpoint-1\n",
      "Configuration saved in ./save/vis-enc/facebook/detr-resnet-50_1e-4_bs32/checkpoint-1/config.json\n",
      "Model weights saved in ./save/vis-enc/facebook/detr-resnet-50_1e-4_bs32/checkpoint-1/pytorch_model.bin\n",
      "Feature extractor saved in ./save/vis-enc/facebook/detr-resnet-50_1e-4_bs32/checkpoint-1/preprocessor_config.json\n",
      "Deleting older checkpoint [save/vis-enc/facebook/detr-resnet-50_1e-4_bs32/checkpoint-9] due to args.save_total_limit\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69090145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb56b304f21c4183a1eba9e32a9ba912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9118, 0.7609, 0.0814, 0.2745],\n",
      "         [0.5883, 0.4441, 0.0124, 0.0885],\n",
      "         [0.6199, 0.8757, 0.1777, 0.2427],\n",
      "         [0.6148, 0.3722, 0.0210, 0.0292],\n",
      "         [0.5719, 0.3782, 0.0193, 0.0347],\n",
      "         [0.6008, 0.8585, 0.1318, 0.2761],\n",
      "         [0.5751, 0.4704, 0.0099, 0.1032],\n",
      "         [0.5336, 0.4668, 0.0111, 0.1033],\n",
      "         [0.5150, 0.3788, 0.0190, 0.0217],\n",
      "         [0.5388, 0.3844, 0.0133, 0.0141],\n",
      "         [0.6841, 0.8982, 0.0832, 0.1985],\n",
      "         [0.9084, 0.8382, 0.1177, 0.3132],\n",
      "         [0.8825, 0.8144, 0.0701, 0.1611],\n",
      "         [0.8086, 0.8571, 0.0466, 0.0577],\n",
      "         [0.6240, 0.9054, 0.0349, 0.1835],\n",
      "         [0.6062, 0.6419, 0.6117, 0.5120],\n",
      "         [0.9097, 0.3595, 0.0287, 0.0179],\n",
      "         [0.7845, 0.3535, 0.0248, 0.0571],\n",
      "         [0.8088, 0.8523, 0.0479, 0.0653],\n",
      "         [0.7328, 0.3664, 0.0162, 0.0325],\n",
      "         [0.8148, 0.9165, 0.1931, 0.1633],\n",
      "         [0.7975, 0.8694, 0.0396, 0.0446],\n",
      "         [0.9171, 0.3685, 0.0422, 0.0217],\n",
      "         [0.9558, 0.9188, 0.0886, 0.1602],\n",
      "         [0.7287, 0.3658, 0.0247, 0.0345],\n",
      "         [0.6686, 0.7273, 0.0764, 0.2380],\n",
      "         [0.9106, 0.4841, 0.0551, 0.2043],\n",
      "         [0.5075, 0.7613, 0.0596, 0.1592],\n",
      "         [0.5750, 0.3804, 0.0172, 0.0319],\n",
      "         [0.9803, 0.6430, 0.0392, 0.0354],\n",
      "         [0.9602, 0.9258, 0.0794, 0.1473],\n",
      "         [0.5787, 0.8700, 0.0829, 0.2494],\n",
      "         [0.8428, 0.9257, 0.1053, 0.1312],\n",
      "         [0.8392, 0.6084, 0.0363, 0.0636],\n",
      "         [0.9035, 0.7803, 0.1140, 0.2603],\n",
      "         [0.5664, 0.4695, 0.0114, 0.1035],\n",
      "         [0.8748, 0.8149, 0.0563, 0.1570],\n",
      "         [0.5110, 0.4615, 0.0263, 0.1103],\n",
      "         [0.5820, 0.7645, 0.0439, 0.0752],\n",
      "         [0.7343, 0.3572, 0.0277, 0.0428],\n",
      "         [0.8207, 0.3610, 0.0280, 0.0564],\n",
      "         [0.8488, 0.3576, 0.0519, 0.0624],\n",
      "         [0.6056, 0.3730, 0.0222, 0.0351],\n",
      "         [0.5775, 0.8718, 0.0847, 0.2478],\n",
      "         [0.9100, 0.4876, 0.0468, 0.1770],\n",
      "         [0.5648, 0.4647, 0.0147, 0.1095],\n",
      "         [0.7452, 0.3581, 0.0167, 0.0545],\n",
      "         [0.6613, 0.9006, 0.0757, 0.1924],\n",
      "         [0.8953, 0.7752, 0.1171, 0.2770],\n",
      "         [0.5380, 0.4682, 0.0131, 0.1058],\n",
      "         [0.5457, 0.3748, 0.0167, 0.0198],\n",
      "         [0.7802, 0.3536, 0.0277, 0.0577],\n",
      "         [0.5763, 0.8792, 0.0816, 0.2326],\n",
      "         [0.5429, 0.4669, 0.0169, 0.1169],\n",
      "         [0.5757, 0.8709, 0.0892, 0.2464],\n",
      "         [0.5082, 0.7674, 0.0639, 0.1794],\n",
      "         [0.9239, 0.6376, 0.0356, 0.0332],\n",
      "         [0.5943, 0.4720, 0.0110, 0.1011],\n",
      "         [0.5814, 0.4706, 0.0099, 0.1024],\n",
      "         [0.7262, 0.8160, 0.3783, 0.3511],\n",
      "         [0.5294, 0.3804, 0.0178, 0.0215],\n",
      "         [0.3881, 0.5485, 0.1622, 0.2924],\n",
      "         [0.9138, 0.3632, 0.0463, 0.0289],\n",
      "         [0.5150, 0.3852, 0.0150, 0.0157],\n",
      "         [0.8455, 0.3557, 0.0428, 0.0518],\n",
      "         [0.5650, 0.8799, 0.0617, 0.2325],\n",
      "         [0.9191, 0.3653, 0.0360, 0.0188],\n",
      "         [0.8881, 0.8180, 0.0985, 0.3260],\n",
      "         [0.5414, 0.4022, 0.0177, 0.0272],\n",
      "         [0.9710, 0.6412, 0.0536, 0.0349],\n",
      "         [0.9059, 0.3658, 0.0269, 0.0251],\n",
      "         [0.6298, 0.8491, 0.1798, 0.2960],\n",
      "         [0.6306, 0.8640, 0.1874, 0.2632],\n",
      "         [0.7846, 0.8737, 0.0328, 0.0467],\n",
      "         [0.6692, 0.9018, 0.0857, 0.1921],\n",
      "         [0.6794, 0.7160, 0.0434, 0.2121],\n",
      "         [0.8809, 0.7946, 0.0654, 0.2066],\n",
      "         [0.9551, 0.9209, 0.0897, 0.1566],\n",
      "         [0.5479, 0.3829, 0.0195, 0.0270],\n",
      "         [0.9125, 0.3684, 0.0450, 0.0248],\n",
      "         [0.8809, 0.8197, 0.0693, 0.1673],\n",
      "         [0.5552, 0.4721, 0.0113, 0.0984],\n",
      "         [0.9558, 0.9207, 0.0881, 0.1569],\n",
      "         [0.5835, 0.3677, 0.0260, 0.0503],\n",
      "         [0.5296, 0.4657, 0.0112, 0.1079],\n",
      "         [0.8759, 0.9211, 0.0894, 0.1474],\n",
      "         [0.9108, 0.7776, 0.1231, 0.2703],\n",
      "         [0.6663, 0.6980, 0.0580, 0.1938],\n",
      "         [0.6252, 0.8760, 0.1824, 0.2407],\n",
      "         [0.6102, 0.7536, 0.0760, 0.0977],\n",
      "         [0.6637, 0.7066, 0.0481, 0.1971],\n",
      "         [0.7816, 0.3505, 0.0317, 0.0579],\n",
      "         [0.6759, 0.8989, 0.0932, 0.1967],\n",
      "         [0.6134, 0.3766, 0.0243, 0.0324],\n",
      "         [0.6639, 0.7035, 0.0399, 0.1838],\n",
      "         [0.6277, 0.8862, 0.1525, 0.2234],\n",
      "         [0.8990, 0.3689, 0.0161, 0.0196],\n",
      "         [0.7307, 0.3414, 0.0292, 0.0343],\n",
      "         [0.9016, 0.8275, 0.1725, 0.3337],\n",
      "         [0.7297, 0.3510, 0.0354, 0.0488]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[[0.0960, 0.8198, 0.0950, 0.3472],\n",
      "         [0.5256, 0.5157, 0.0297, 0.0955],\n",
      "         [0.2071, 0.7962, 0.0863, 0.3551],\n",
      "         [0.6284, 0.2577, 0.0416, 0.0304],\n",
      "         [0.5215, 0.3781, 0.0281, 0.1451],\n",
      "         [0.5122, 0.6748, 0.0407, 0.2552],\n",
      "         [0.5536, 0.5692, 0.0216, 0.0346],\n",
      "         [0.3772, 0.6730, 0.0625, 0.1973],\n",
      "         [0.4640, 0.3496, 0.0230, 0.1137],\n",
      "         [0.4732, 0.2784, 0.0242, 0.0225],\n",
      "         [0.6967, 0.8645, 0.0970, 0.2457],\n",
      "         [0.1536, 0.8170, 0.0848, 0.3490],\n",
      "         [0.8230, 0.8053, 0.3547, 0.3718],\n",
      "         [0.7575, 0.7756, 0.0164, 0.0512],\n",
      "         [0.5330, 0.8960, 0.0138, 0.0438],\n",
      "         [0.1988, 0.7783, 0.1151, 0.3481],\n",
      "         [0.2084, 0.6228, 0.0358, 0.0505],\n",
      "         [0.7222, 0.3913, 0.0166, 0.2100],\n",
      "         [0.7466, 0.7480, 0.0152, 0.0424],\n",
      "         [0.6299, 0.6707, 0.1997, 0.2117],\n",
      "         [0.8093, 0.8176, 0.3286, 0.3483],\n",
      "         [0.7460, 0.7504, 0.0157, 0.0406],\n",
      "         [0.8635, 0.4848, 0.1140, 0.4615],\n",
      "         [0.9062, 0.8422, 0.1867, 0.2858],\n",
      "         [0.6860, 0.5651, 0.0466, 0.1175],\n",
      "         [0.6764, 0.6376, 0.0370, 0.0596],\n",
      "         [0.8513, 0.6327, 0.0671, 0.1493],\n",
      "         [0.4426, 0.6580, 0.0889, 0.2518],\n",
      "         [0.4985, 0.3782, 0.0168, 0.1328],\n",
      "         [0.0634, 0.8204, 0.0706, 0.3373],\n",
      "         [0.0584, 0.8285, 0.0616, 0.3381],\n",
      "         [0.5338, 0.8375, 0.0222, 0.0421],\n",
      "         [0.1613, 0.8178, 0.0671, 0.3475],\n",
      "         [0.7613, 0.7409, 0.0229, 0.0455],\n",
      "         [0.8700, 0.7416, 0.1047, 0.3625],\n",
      "         [0.5221, 0.5164, 0.0314, 0.0922],\n",
      "         [0.7568, 0.6022, 0.0530, 0.1312],\n",
      "         [0.3657, 0.6445, 0.0608, 0.1443],\n",
      "         [0.5219, 0.8264, 0.0163, 0.0395],\n",
      "         [0.6850, 0.3843, 0.0159, 0.1856],\n",
      "         [0.7499, 0.3944, 0.0171, 0.2283],\n",
      "         [0.7646, 0.3931, 0.0179, 0.2306],\n",
      "         [0.6287, 0.3813, 0.0464, 0.1692],\n",
      "         [0.4426, 0.6605, 0.1115, 0.2574],\n",
      "         [0.1595, 0.7706, 0.1748, 0.2909],\n",
      "         [0.5028, 0.6527, 0.0442, 0.2372],\n",
      "         [0.7065, 0.3894, 0.0162, 0.1998],\n",
      "         [0.6282, 0.6881, 0.0165, 0.0659],\n",
      "         [0.0742, 0.8233, 0.0932, 0.3438],\n",
      "         [0.5015, 0.5539, 0.0257, 0.0459],\n",
      "         [0.4957, 0.2705, 0.0296, 0.0389],\n",
      "         [0.6952, 0.3873, 0.0158, 0.1932],\n",
      "         [0.5288, 0.8986, 0.0138, 0.0425],\n",
      "         [0.4732, 0.3768, 0.0143, 0.1270],\n",
      "         [0.2029, 0.8017, 0.0690, 0.3331],\n",
      "         [0.3735, 0.6753, 0.0624, 0.2080],\n",
      "         [0.1792, 0.7682, 0.0542, 0.2516],\n",
      "         [0.5710, 0.5764, 0.0237, 0.0418],\n",
      "         [0.5239, 0.5156, 0.0281, 0.0929],\n",
      "         [0.7529, 0.7717, 0.4912, 0.4392],\n",
      "         [0.4639, 0.3767, 0.0144, 0.1233],\n",
      "         [0.4036, 0.6753, 0.0550, 0.2086],\n",
      "         [0.8733, 0.2136, 0.0886, 0.0452],\n",
      "         [0.4398, 0.3766, 0.0180, 0.1168],\n",
      "         [0.6895, 0.2237, 0.0461, 0.0616],\n",
      "         [0.4066, 0.6931, 0.0462, 0.1852],\n",
      "         [0.0739, 0.7055, 0.0783, 0.1320],\n",
      "         [0.1449, 0.8042, 0.2192, 0.3717],\n",
      "         [0.4591, 0.3759, 0.0153, 0.1234],\n",
      "         [0.8951, 0.7390, 0.0716, 0.0889],\n",
      "         [0.7864, 0.3915, 0.0231, 0.2310],\n",
      "         [0.6193, 0.7730, 0.2463, 0.4169],\n",
      "         [0.7481, 0.7865, 0.4963, 0.4102],\n",
      "         [0.7384, 0.7501, 0.0125, 0.0402],\n",
      "         [0.6534, 0.9344, 0.1465, 0.1270],\n",
      "         [0.6867, 0.5663, 0.0499, 0.1202],\n",
      "         [0.2204, 0.7774, 0.0723, 0.3299],\n",
      "         [0.0772, 0.8291, 0.0921, 0.3354],\n",
      "         [0.4858, 0.3768, 0.0148, 0.1296],\n",
      "         [0.7918, 0.3907, 0.0206, 0.2329],\n",
      "         [0.8622, 0.8037, 0.0987, 0.2531],\n",
      "         [0.5109, 0.5577, 0.0249, 0.0407],\n",
      "         [0.9701, 0.7865, 0.0595, 0.1358],\n",
      "         [0.6309, 0.2278, 0.0510, 0.0647],\n",
      "         [0.4420, 0.4960, 0.0263, 0.1177],\n",
      "         [0.7581, 0.7982, 0.0213, 0.0299],\n",
      "         [0.8392, 0.7224, 0.0591, 0.0830],\n",
      "         [0.6859, 0.5661, 0.0490, 0.1168],\n",
      "         [0.6541, 0.9185, 0.1542, 0.1576],\n",
      "         [0.5574, 0.5716, 0.0206, 0.0335],\n",
      "         [0.6287, 0.5620, 0.0371, 0.1248],\n",
      "         [0.7576, 0.5983, 0.0559, 0.1306],\n",
      "         [0.6939, 0.8651, 0.1021, 0.2430],\n",
      "         [0.6312, 0.5589, 0.0328, 0.1150],\n",
      "         [0.6343, 0.6772, 0.0153, 0.0522],\n",
      "         [0.5907, 0.9009, 0.0493, 0.1165],\n",
      "         [0.1824, 0.6354, 0.0548, 0.0603],\n",
      "         [0.6729, 0.2557, 0.0421, 0.0299],\n",
      "         [0.7992, 0.7970, 0.3876, 0.3870],\n",
      "         [0.6692, 0.3822, 0.0165, 0.1785]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0219, 0.7749, 0.0419, 0.2410],\n",
      "         [0.4140, 0.5272, 0.0134, 0.2696],\n",
      "         [0.2849, 0.8442, 0.0712, 0.2953],\n",
      "         [0.4216, 0.5287, 0.0136, 0.2733],\n",
      "         [0.3524, 0.4890, 0.0458, 0.2467],\n",
      "         [0.3383, 0.6698, 0.0830, 0.6248],\n",
      "         [0.3738, 0.5700, 0.0205, 0.0252],\n",
      "         [0.3088, 0.7279, 0.0416, 0.1048],\n",
      "         [0.2485, 0.3529, 0.0321, 0.0513],\n",
      "         [0.2027, 0.5481, 0.2987, 0.4731],\n",
      "         [0.6703, 0.8159, 0.0301, 0.0974],\n",
      "         [0.2260, 0.6559, 0.0914, 0.6577],\n",
      "         [0.7366, 0.7497, 0.1468, 0.4794],\n",
      "         [0.7351, 0.7814, 0.1351, 0.4209],\n",
      "         [0.3710, 0.8812, 0.0290, 0.1233],\n",
      "         [0.2268, 0.5288, 0.0748, 0.3950],\n",
      "         [0.2160, 0.4230, 0.0707, 0.1651],\n",
      "         [0.6573, 0.5449, 0.0360, 0.0970],\n",
      "         [0.6676, 0.6825, 0.0690, 0.3605],\n",
      "         [0.1785, 0.5417, 0.3499, 0.5014],\n",
      "         [0.7222, 0.7511, 0.1678, 0.4832],\n",
      "         [0.7259, 0.7498, 0.1626, 0.4859],\n",
      "         [0.0307, 0.6022, 0.0604, 0.6442],\n",
      "         [0.0194, 0.8729, 0.0368, 0.1435],\n",
      "         [0.6627, 0.6372, 0.0793, 0.4284],\n",
      "         [0.6280, 0.6973, 0.1665, 0.5929],\n",
      "         [0.4204, 0.6373, 0.7905, 0.7119],\n",
      "         [0.3341, 0.6736, 0.0765, 0.6196],\n",
      "         [0.3758, 0.5564, 0.0179, 0.0162],\n",
      "         [0.0278, 0.5919, 0.0548, 0.5949],\n",
      "         [0.0206, 0.6362, 0.0409, 0.6266],\n",
      "         [0.3449, 0.8323, 0.0673, 0.3072],\n",
      "         [0.2526, 0.8781, 0.0851, 0.2355],\n",
      "         [0.7405, 0.6374, 0.1143, 0.2396],\n",
      "         [0.4813, 0.6523, 0.6494, 0.6734],\n",
      "         [0.3735, 0.5652, 0.0195, 0.0273],\n",
      "         [0.7326, 0.7525, 0.1459, 0.4784],\n",
      "         [0.2644, 0.5099, 0.0558, 0.3553],\n",
      "         [0.3270, 0.7224, 0.0367, 0.1038],\n",
      "         [0.6441, 0.5564, 0.0263, 0.0900],\n",
      "         [0.6575, 0.5484, 0.0435, 0.1110],\n",
      "         [0.7315, 0.6528, 0.1721, 0.6742],\n",
      "         [0.4169, 0.5277, 0.0157, 0.2725],\n",
      "         [0.3239, 0.8359, 0.0824, 0.3107],\n",
      "         [0.2208, 0.5305, 0.0634, 0.3809],\n",
      "         [0.3458, 0.4917, 0.0535, 0.2506],\n",
      "         [0.6615, 0.6717, 0.0549, 0.3640],\n",
      "         [0.5291, 0.8092, 0.1050, 0.3624],\n",
      "         [0.0678, 0.6022, 0.1345, 0.6459],\n",
      "         [0.3162, 0.6217, 0.0383, 0.0633],\n",
      "         [0.3408, 0.4969, 0.0365, 0.2533],\n",
      "         [0.6585, 0.6367, 0.0998, 0.4289],\n",
      "         [0.3277, 0.8366, 0.1057, 0.3096],\n",
      "         [0.3404, 0.4948, 0.0422, 0.2485],\n",
      "         [0.2882, 0.8454, 0.0648, 0.2945],\n",
      "         [0.2560, 0.6616, 0.1507, 0.6505],\n",
      "         [0.2251, 0.5315, 0.0640, 0.3779],\n",
      "         [0.4217, 0.5272, 0.0139, 0.2715],\n",
      "         [0.4160, 0.5423, 0.0162, 0.3013],\n",
      "         [0.4302, 0.5813, 0.8485, 0.8211],\n",
      "         [0.3195, 0.5033, 0.0374, 0.2633],\n",
      "         [0.2489, 0.6515, 0.2192, 0.6678],\n",
      "         [0.4203, 0.6331, 0.8380, 0.7123],\n",
      "         [0.2552, 0.5098, 0.0594, 0.3520],\n",
      "         [0.4069, 0.6927, 0.8028, 0.5995],\n",
      "         [0.2973, 0.8430, 0.0704, 0.2985],\n",
      "         [0.0657, 0.5741, 0.1303, 0.5981],\n",
      "         [0.0989, 0.5901, 0.1987, 0.6320],\n",
      "         [0.2841, 0.5053, 0.0420, 0.3177],\n",
      "         [0.0182, 0.8153, 0.0358, 0.1087],\n",
      "         [0.7378, 0.6303, 0.1260, 0.2352],\n",
      "         [0.6132, 0.6820, 0.1424, 0.6220],\n",
      "         [0.4705, 0.6466, 0.6918, 0.6817],\n",
      "         [0.6583, 0.6749, 0.0479, 0.3589],\n",
      "         [0.5421, 0.8064, 0.1131, 0.3598],\n",
      "         [0.6651, 0.6530, 0.0848, 0.4567],\n",
      "         [0.2212, 0.5297, 0.0736, 0.3904],\n",
      "         [0.0229, 0.8321, 0.0278, 0.0802],\n",
      "         [0.3450, 0.4955, 0.0403, 0.2414],\n",
      "         [0.2127, 0.5325, 0.0554, 0.3765],\n",
      "         [0.7374, 0.7542, 0.1355, 0.4721],\n",
      "         [0.3500, 0.5614, 0.0618, 0.3920],\n",
      "         [0.0166, 0.8426, 0.0333, 0.2616],\n",
      "         [0.4126, 0.5301, 0.0135, 0.2727],\n",
      "         [0.2665, 0.5085, 0.0411, 0.3458],\n",
      "         [0.7368, 0.8297, 0.1282, 0.3260],\n",
      "         [0.7467, 0.7836, 0.1103, 0.4160],\n",
      "         [0.6263, 0.6868, 0.1522, 0.6107],\n",
      "         [0.3161, 0.8318, 0.1406, 0.3172],\n",
      "         [0.3762, 0.6254, 0.0273, 0.1215],\n",
      "         [0.5936, 0.6837, 0.0983, 0.6140],\n",
      "         [0.7206, 0.7083, 0.1952, 0.5701],\n",
      "         [0.6279, 0.6899, 0.1897, 0.6099],\n",
      "         [0.4269, 0.5252, 0.0140, 0.2676],\n",
      "         [0.6420, 0.5554, 0.0272, 0.1091],\n",
      "         [0.4810, 0.7636, 0.2203, 0.4524],\n",
      "         [0.2234, 0.5152, 0.0650, 0.3493],\n",
      "         [0.5990, 0.5308, 0.0903, 0.2790],\n",
      "         [0.6889, 0.6999, 0.2656, 0.5876],\n",
      "         [0.4289, 0.5103, 0.0172, 0.2313]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(proc_datasets[\"valid\"])):\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=batch[\"pixel_values\"].unsqueeze(dim=0).to(device),\n",
    "                          pixel_mask=batch[\"pixel_mask\"].unsqueeze(dim=0).to(device))\n",
    "    print(outputs.pred_boxes.shape, outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7aa432b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 4]), torch.Size([1, 100, 256]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pred_boxes.shape, outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1db01dd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/trainer.py:2861\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2858\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2860\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2861\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2864\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2865\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2866\u001b[0m     speed_metrics(\n\u001b[1;32m   2867\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     )\n\u001b[1;32m   2872\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/trainer.py:2965\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2962\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2965\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2966\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mHolyTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_loss_only:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer_pt_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnested_detach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(logits) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     76\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:159\u001b[0m, in \u001b[0;36mnested_detach\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetach `tensors` (even if it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a nested list/tuple of tensors).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_detach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:159\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetach `tensors` (even if it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a nested list/tuple of tensors).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_detach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:160\u001b[0m, in \u001b[0;36mnested_detach\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_detach(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "trainer.predict(proc_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57d8002d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'getImgIds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m iou_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m coco_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mCocoEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_types\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# initialize evaluator with ground truths\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/projects/ambiguous-mm-dialogue/detr/datasets/coco_eval.py:31\u001b[0m, in \u001b[0;36mCocoEvaluator.__init__\u001b[0;34m(self, coco_gt, iou_types)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_eval \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iou_type \u001b[38;5;129;01min\u001b[39;00m iou_types:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_eval[iou_type] \u001b[38;5;241m=\u001b[39m \u001b[43mCOCOeval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_gt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miouType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miou_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_imgs \u001b[38;5;241m=\u001b[39m {k: [] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m iou_types}\n",
      "File \u001b[0;32m~/anaconda3/envs/ambican/lib/python3.10/site-packages/pycocotools/cocoeval.py:80\u001b[0m, in \u001b[0;36mCOCOeval.__init__\u001b[0;34m(self, cocoGt, cocoDt, iouType)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mious \u001b[38;5;241m=\u001b[39m {}                      \u001b[38;5;66;03m# ious between all gts and dts\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cocoGt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mcocoGt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetImgIds\u001b[49m())\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mcatIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(cocoGt\u001b[38;5;241m.\u001b[39mgetCatIds())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'getImgIds'"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(proc_datasets[\"valid\"], iou_types) # initialize evaluator with ground truths\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(proc_datasets[\"valid\"])):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "    print(outputs)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not return_dict:\n",
    "    if auxiliary_outputs is not None:\n",
    "        output = (logits, pred_boxes) + auxiliary_outputs + outputs\n",
    "    else:\n",
    "        output = (logits, pred_boxes) + outputs\n",
    "    return ((loss, loss_dict) + output) if loss is not None else output\n",
    "\n",
    "return DetrObjectDetectionOutput(\n",
    "    loss=loss,\n",
    "    loss_dict=loss_dict,\n",
    "    logits=logits,\n",
    "    pred_boxes=pred_boxes,\n",
    "    auxiliary_outputs=auxiliary_outputs,\n",
    "    last_hidden_state=outputs.last_hidden_state,\n",
    "    decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "    decoder_attentions=outputs.decoder_attentions,\n",
    "    cross_attentions=outputs.cross_attentions,\n",
    "    encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "    encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "    encoder_attentions=outputs.encoder_attentions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e838c43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 4))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_datasets[\"valid\"][0][\"labels\"][\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b72ac361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.4851,  0.4851,  0.4851,  ...,  1.2214,  1.2214,  1.2385],\n",
       "          [ 0.4851,  0.4851,  0.4851,  ...,  1.2214,  1.2385,  1.2385],\n",
       "          [ 0.4851,  0.4851,  0.4851,  ...,  1.2214,  1.2385,  1.2385],\n",
       "          ...,\n",
       "          [-0.9363, -0.9020, -0.8335,  ..., -0.6109, -0.6109, -0.6109],\n",
       "          [-0.9877, -0.9534, -0.8678,  ..., -0.6109, -0.6109, -0.6109],\n",
       "          [-1.0219, -0.9877, -0.9192,  ..., -0.6109, -0.6109, -0.6109]],\n",
       " \n",
       "         [[ 0.6254,  0.6254,  0.6254,  ...,  1.3782,  1.3782,  1.3957],\n",
       "          [ 0.6254,  0.6254,  0.6254,  ...,  1.3782,  1.3957,  1.3957],\n",
       "          [ 0.6254,  0.6254,  0.6254,  ...,  1.3782,  1.3957,  1.3957],\n",
       "          ...,\n",
       "          [-0.8102, -0.7752, -0.6877,  ..., -0.4951, -0.4951, -0.4951],\n",
       "          [-0.8627, -0.8102, -0.7227,  ..., -0.4951, -0.4951, -0.4951],\n",
       "          [-0.8978, -0.8452, -0.7577,  ..., -0.4951, -0.4951, -0.4951]],\n",
       " \n",
       "         [[ 0.8448,  0.8448,  0.8448,  ...,  1.5942,  1.5942,  1.6117],\n",
       "          [ 0.8448,  0.8448,  0.8448,  ...,  1.5942,  1.6117,  1.6117],\n",
       "          [ 0.8448,  0.8448,  0.8448,  ...,  1.5942,  1.6117,  1.6117],\n",
       "          ...,\n",
       "          [-0.6018, -0.5670, -0.4973,  ..., -0.2707, -0.2707, -0.2707],\n",
       "          [-0.6541, -0.6193, -0.5321,  ..., -0.2707, -0.2707, -0.2707],\n",
       "          [-0.6890, -0.6541, -0.5844,  ..., -0.2707, -0.2707, -0.2707]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': {'boxes': tensor([[0.8054, 0.2317, 0.1442, 0.1167],\n",
       "          [0.8979, 0.4264, 0.0854, 0.2923],\n",
       "          [0.3765, 0.3266, 0.1559, 0.1799],\n",
       "          [0.4833, 0.3653, 0.1193, 0.1352],\n",
       "          [0.5602, 0.3746, 0.0970, 0.0818],\n",
       "          [0.6498, 0.3986, 0.0726, 0.1320],\n",
       "          [0.6522, 0.3866, 0.0329, 0.0316],\n",
       "          [0.6389, 0.4029, 0.0636, 0.0338],\n",
       "          [0.8073, 0.5698, 0.3324, 0.7361],\n",
       "          [0.7511, 0.6510, 0.1999, 0.5998],\n",
       "          [0.5480, 0.5971, 0.0938, 0.0840],\n",
       "          [0.5705, 0.5823, 0.0901, 0.0589],\n",
       "          [0.5745, 0.5840, 0.0960, 0.0883],\n",
       "          [0.5803, 0.5862, 0.0790, 0.0752],\n",
       "          [0.0233, 0.6549, 0.0021, 0.0033]]),\n",
       "  'class_labels': tensor([9, 9, 6, 0, 0, 9, 9, 6, 9, 9, 6, 6, 6, 6, 7]),\n",
       "  'image_id': tensor([89]),\n",
       "  'area': tensor([863136., 863136., 863136., 863136., 863136., 863136., 863136., 863136.,\n",
       "          863136., 863136., 863136., 863136., 863136., 863136., 863136.]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'orig_size': tensor([ 917, 1886]),\n",
       "  'size': tensor([ 648, 1332])}}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_datasets[\"valid\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce43e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
